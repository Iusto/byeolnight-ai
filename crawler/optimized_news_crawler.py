#!/usr/bin/env python3
"""
ìµœì í™”ëœ ìš°ì£¼ ë‰´ìŠ¤ í¬ë¡¤ë§ (ë‚ ì§œ í•„í„°ë§ í¬í•¨)
"""
import requests
from bs4 import BeautifulSoup
import logging
from datetime import datetime, timedelta
from dateutil import parser
import re

logger = logging.getLogger(__name__)

def is_recent_news(pub_date_str, max_days=7):
    """ë‰´ìŠ¤ê°€ ìµœê·¼ Nì¼ ì´ë‚´ì¸ì§€ í™•ì¸"""
    try:
        if not pub_date_str:
            return True  # ë‚ ì§œ ì •ë³´ê°€ ì—†ìœ¼ë©´ í¬í•¨
        
        # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ íŒŒì‹±
        pub_date = parser.parse(pub_date_str)
        cutoff_date = datetime.now() - timedelta(days=max_days)
        
        return pub_date.replace(tzinfo=None) >= cutoff_date
    except:
        return True  # íŒŒì‹± ì‹¤íŒ¨ì‹œ í¬í•¨

def is_valid_content(title, content):
    """ìœ íš¨í•œ ë‰´ìŠ¤ ì½˜í…ì¸ ì¸ì§€ í™•ì¸"""
    if not title or len(title.strip()) < 10:
        return False
    if content and len(content.strip()) < 20:
        return False
    return True

def crawl_google_news_optimized():
    """ìµœì í™”ëœ êµ¬ê¸€ ë‰´ìŠ¤ í¬ë¡¤ë§"""
    try:
        import random
        import time
        
        # ìºì‹œ ë°©ì§€ë¥¼ ìœ„í•œ ëœë¤ íŒŒë¼ë¯¸í„° ì¶”ê°€
        cache_buster = int(time.time()) + random.randint(1, 1000)
        url = f"https://news.google.com/rss/search?q=ìš°ì£¼+ë‰´ìŠ¤&hl=ko&gl=KR&ceid=KR:ko&_cb={cache_buster}"
        headers = {
            'User-Agent': f'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/{random.randint(100,120)}.0.0.0 Safari/537.36',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache'
        }
        
        resp = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(resp.content, "xml")
        
        articles = []
        seen_titles = set()  # ì¤‘ë³µ ì œê±°ìš©
        seen_keywords = []  # ìœ ì‚¬ ë‚´ìš© ì œê±°ìš©
        
        for item in soup.find_all("item")[:10]:
            title_tag = item.find("title")
            link_tag = item.find("link")
            source_tag = item.find("source")
            pub_date_tag = item.find("pubDate")
            description_tag = item.find("description")  # RSS ì„¤ëª… ì¶”ê°€
            
            if not title_tag:
                continue
                
            title = title_tag.get_text(strip=True)
            pub_date = pub_date_tag.get_text(strip=True) if pub_date_tag else ""
            rss_description = description_tag.get_text(strip=True) if description_tag else ""
            
            # ë‚ ì§œ í•„í„°ë§ (ìµœê·¼ 7ì¼)
            if not is_recent_news(pub_date, max_days=7):
                logger.debug(f"ì˜¤ë˜ëœ ë‰´ìŠ¤ ì œì™¸: {title[:30]}...")
                continue
            
            # ì œëª©ì—ì„œ ì¶œì²˜ ì™„ì „ ì œê±° (ì˜ˆ: [í•œêµ­ëŒ€í•™ì‹ ë¬¸], - í•œêµ­ëŒ€í•™ì‹ ë¬¸ ë“±)
            clean_title = re.sub(r'^\[.*?\]\s*', '', title).strip()
            clean_title = re.sub(r'\s*-\s*[ê°€-í£A-Za-z0-9\s]+$', '', clean_title).strip()
            
            # ì¤‘ë³µ ë° ìœ ì‚¬ ì œëª© ì œê±°
            if clean_title in seen_titles:
                continue
            
            # ìœ ì‚¬ ì œëª© ê²€ì‚¬ (NASA, ë„·í”Œë¦­ìŠ¤ ë“± í‚¤ì›Œë“œ ê¸°ë°˜)
            title_keywords = set(clean_title.lower().split())
            is_similar = False
            for seen_keyword_set in seen_keywords:
                # 50% ì´ìƒ ê³µí†µ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ìœ ì‚¬í•œ ê²ƒìœ¼ë¡œ íŒë‹¨
                common_words = title_keywords & seen_keyword_set
                if len(common_words) >= 2 and len(common_words) / len(title_keywords) > 0.5:
                    is_similar = True
                    logger.debug(f"ìœ ì‚¬ ì œëª© ì œì™¸: {clean_title[:30]}...")
                    break
            
            if is_similar:
                continue
                
            seen_titles.add(clean_title)
            seen_keywords.append(title_keywords)
            
            link = link_tag.get_text(strip=True) if link_tag else ""
            source = source_tag.get_text(strip=True) if source_tag else "êµ¬ê¸€ë‰´ìŠ¤"
            
            # ìƒì„¸ ë‚´ìš© ì¶”ì¶œ (ê°•í™”ëœ ë°©ë²•)
            content, image_url = get_article_content(link, rss_description, clean_title)
            
            # ëª¨ë“  ê¸°ì‚¬ì—ì„œ Selenium í’ˆì§ˆ ê°œì„  ì‹œë„ (ê°•í™”)
            selenium_attempted = False
            if True:
                try:
                    from crawler.selenium_enhancer import enhance_article_with_selenium, is_selenium_available
                    if is_selenium_available():
                        logger.info(f"Seleniumìœ¼ë¡œ í’ˆì§ˆ ê°œì„  ì‹œë„: {clean_title[:30]}...")
                        enhanced_content, enhanced_image = enhance_article_with_selenium(link, clean_title)
                        selenium_attempted = True
                        
                        # ë” ì—„ê²©í•œ í’ˆì§ˆ ê¸°ì¤€ ì ìš©
                        if enhanced_content and len(str(enhanced_content)) > 500:
                            logger.info(f"Selenium ì„±ê³µ: {len(enhanced_content)}ì ì¶”ì¶œ (ê¸°ì¡´: {len(content if content else '')}ì)")
                            content = enhanced_content
                            if enhanced_image:
                                image_url = enhanced_image
                        elif enhanced_content and len(str(enhanced_content)) > len(str(content) if content else 0):
                            logger.info(f"Selenium ë¶€ë¶„ ì„±ê³µ: {len(enhanced_content)}ì ì¶”ì¶œ")
                            content = enhanced_content
                            if enhanced_image:
                                image_url = enhanced_image
                        else:
                            logger.warning(f"Selenium ê²°ê³¼ ë¶€ì¡±: {len(enhanced_content if enhanced_content else 0)}ì")
                except Exception as e:
                    logger.error(f"Selenium ì˜¤ë¥˜: {e}")
                    selenium_attempted = True
            
            # ë¬´ì˜ë¯¸í•œ ì½˜í…ì¸  í•„í„°ë§
            if content and any(skip_text in content for skip_text in [
                'Google ë‰´ìŠ¤ê°€ ì „ì„¸ê³„', 'ì „ì„¸ê³„ ë§¤ì²´ë¡œë¶€í„°', 
                'ì¢…í•©í•œ ìµœì‹  ë‰´ìŠ¤', 'ë‰´ìŠ¤ ì†ŒìŠ¤', 'ë‰´ìŠ¤ ì œê³µì—…ì²´'
            ]):
                content = ""  # ë¬´ì˜ë¯¸í•œ ì½˜í…ì¸  ì œê±°
            
            # ì½˜í…ì¸  ìœ íš¨ì„± ê²€ì‚¬ (ë„ˆë¬´ ì—„ê²©í•˜ì§€ ì•Šê²Œ)
            if not clean_title or len(clean_title.strip()) < 5:
                logger.debug(f"ìœ íš¨í•˜ì§€ ì•Šì€ ì œëª© ì œì™¸: {title[:30]}...")
                continue
            
            # AI í‰ê°€ ë° ìš”ì•½
            from ai.news_evaluator import evaluate_news_article
            evaluation = evaluate_news_article(clean_title, content, link)
            
            if evaluation["evaluation"] == "REJECT":
                logger.debug(f"AI í‰ê°€ ê±°ë¶€: {title[:30]}... - {evaluation.get('reason', '')}")
                continue
            
            # í’ë¶€í•œ ì½˜í…ì¸  ìƒì„± (ê°œì„ ëœ ë²„ì „)
            full_content = f"{source}ì—ì„œ ë³´ë„í•œ ìš°ì£¼ ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤ì…ë‹ˆë‹¤.\n\n"
            
            # ì‹¤ì œ ê¸°ì‚¬ ë‚´ìš© ìš°ì„  ë°°ì¹˜ (í’ˆì§ˆ ê°œì„ )
            if content and len(content.strip()) > 800:
                # ê³ í’ˆì§ˆ ë‚´ìš© (ì „ì²´ í‘œì‹œ)
                full_content += f"ğŸ“° ê¸°ì‚¬ ë‚´ìš©:\n{content}\n\n"
            elif content and len(content.strip()) > 400:
                # ì¤‘ê°„ í’ˆì§ˆ ë‚´ìš©
                full_content += f"ğŸ“° ê¸°ì‚¬ ë‚´ìš©:\n{content}\n\n"
            elif content and len(content.strip()) > 200:
                # ê¸°ë³¸ í’ˆì§ˆ ë‚´ìš©
                full_content += f"ğŸ“° ê¸°ì‚¬ ë‚´ìš©:\n{content}\n\n"
            elif content and len(content.strip()) > 100:
                # ì§§ì€ ë‚´ìš©
                clean_content = content.replace(clean_title, '').replace(source, '').strip()
                if len(clean_content) > 50:
                    full_content += f"ğŸ“° ê¸°ì‚¬ ìš”ì•½: {clean_content}\n\n"
                else:
                    full_content += f"ğŸ“° ê¸°ì‚¬ ë‚´ìš©: {content}\n\n"
            else:
                # ë‚´ìš©ì´ ë¶€ì¡±í•  ë•Œ AI ìš”ì•½ í™œìš©
                if evaluation.get("summary") and len(evaluation["summary"]) > 50:
                    full_content += f"ğŸ“° ê¸°ì‚¬ ë‚´ìš©: {evaluation['summary']}\n\n"
                else:
                    # ì œëª© ê¸°ë°˜ ì„¤ëª… ìƒì„±
                    if 'ë³´ë ¹' in clean_title and 'ëŒ€í‘œ' in clean_title:
                        full_content += f"ğŸ“° ê¸°ì‚¬ ë‚´ìš©: ë³´ë ¹ ê¹€ì •ê·  ëŒ€í‘œê°€ ìš°ì£¼ ì‚¬ì—… í™•ì¥ì— ëŒ€í•œ í¬ë¶€ë¥¼ ë°í˜”ìŠµë‹ˆë‹¤. í•œêµ­ì˜ ìš°ì£¼ ì‚°ì—… ë°œì „ì— ëŒ€í•œ ì˜ì§€ë¥¼ í‘œëª…í•œ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.\n\n"
                    elif 'NASA' in clean_title and 'ë„·í”Œë¦­ìŠ¤' in clean_title:
                        full_content += f"ğŸ“° ê¸°ì‚¬ ë‚´ìš©: NASAê°€ ë„·í”Œë¦­ìŠ¤ì™€ í˜‘ë ¥í•˜ì—¬ ìš°ì£¼ ì˜ìƒ ì½˜í…ì¸ ë¥¼ ì œê³µí•˜ê¸°ë¡œ í–ˆìŠµë‹ˆë‹¤. ì¼ë°˜ì¸ë“¤ì´ ìš°ì£¼ íƒì‚¬ì˜ ì§œë¦¿í•¨ì„ ë” ì‰½ê²Œ ëŠë‚„ ìˆ˜ ìˆê²Œ ë  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤.\n\n"
                    elif 'ì´ˆë“±ìƒ' in clean_title and 'ISS' in clean_title:
                        full_content += f"ğŸ“° ê¸°ì‚¬ ë‚´ìš©: í•œêµ­ ì´ˆë“±ìƒì˜ ìš°ì£¼ ê¿ˆì´ êµ­ì œìš°ì£¼ì •ê±°ì¥(ISS)ì—ì„œ ìƒì¤‘ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ì–´ë¦°ì´ë“¤ì˜ ìš°ì£¼ì— ëŒ€í•œ ê¿ˆê³¼ í¬ë§ì„ ë³´ì—¬ì£¼ëŠ” ì˜ë¯¸ ìˆëŠ” ì‚¬ê±´ì…ë‹ˆë‹¤.\n\n"
                    else:
                        full_content += f"ğŸ“° ê¸°ì‚¬ ì£¼ì œ: {clean_title}ì— ëŒ€í•œ ìš°ì£¼ ê³¼í•™ ì†Œì‹ì…ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ì›ë¬¸ì—ì„œ í™•ì¸í•˜ì„¸ìš”.\n\n"
            
            # AI ìš”ì•½ ì¶”ê°€ (ë³´ì¡°ì  ì—­í• )
            if evaluation.get("summary") and len(evaluation["summary"]) > 30 and not content:
                full_content += f"ğŸ¤– AI ìš”ì•½: {evaluation['summary']}\n\n"
            
            # ì´ë¯¸ì§€ ì¶”ê°€
            if image_url:
                full_content += f"ğŸ–¼ï¸ ê´€ë ¨ ì´ë¯¸ì§€: {image_url}\n\n"
            
            if pub_date:
                full_content += f"ğŸ“… ë°œí–‰ì¼: {pub_date}\n"
            
            # AI í‚¤ì›Œë“œ ì¶”ê°€
            if evaluation.get("keywords") and len(evaluation["keywords"]) > 0:
                keywords_str = ", ".join(evaluation["keywords"][:3])
                full_content += f"ğŸ·ï¸ í•µì‹¬ í‚¤ì›Œë“œ: {keywords_str}\n"
            
            full_content += f"ğŸ”— ì›ë¬¸ ë§í¬: {link}\nğŸŒŒ ì¶œì²˜: {source}"
            
            articles.append({
                "title": clean_title,
                "content": full_content,
                "source": "GoogleNews",
                "published_at": pub_date,
                "url": link,
                "ai_evaluation": evaluation
            })
            
            if len(articles) >= 3:  # ìµœëŒ€ 3ê°œ
                break
        
        logger.info(f"êµ¬ê¸€ ë‰´ìŠ¤ ìµœì‹  ìš°ì£¼ ë‰´ìŠ¤ {len(articles)}ê°œ ìˆ˜ì§‘")
        return articles
        
    except Exception as e:
        logger.error(f"êµ¬ê¸€ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

def get_article_content(url, rss_description="", clean_title=""):
    """ê¸°ì‚¬ URLì—ì„œ ìƒì„¸ ë‚´ìš© ì¶”ì¶œ (ê°•í™”ë²„ì „)"""
    try:
        import time
        import urllib.parse
        import base64
        
        original_url = url
        
        # êµ¬ê¸€ ë‰´ìŠ¤ URL ì²˜ë¦¬ ê°•í™”
        if 'news.google.com' in url:
            # ë°©ë²• 1: URL íŒŒë¼ë¯¸í„°ì—ì„œ ì¶”ì¶œ
            try:
                if 'url=' in url:
                    parsed = urllib.parse.parse_qs(urllib.parse.urlparse(url).query)
                    if 'url' in parsed:
                        url = parsed['url'][0]
                        logger.info(f"URL íŒŒë¼ë¯¸í„°ì—ì„œ ì¶”ì¶œ: {url[:100]}...")
            except:
                pass
            
            # ë°©ë²• 2: Base64 ë””ì½”ë”© ì‹œë„
            if 'news.google.com' in url:
                try:
                    # êµ¬ê¸€ ë‰´ìŠ¤ URLì—ì„œ ì¸ì½”ë”©ëœ ë¶€ë¶„ ì¶”ì¶œ
                    if '/articles/' in url:
                        article_id = url.split('/articles/')[-1].split('?')[0]
                        # Base64 ë””ì½”ë”© ì‹œë„
                        try:
                            decoded = base64.b64decode(article_id + '==').decode('utf-8', errors='ignore')
                            if 'http' in decoded:
                                import re
                                urls = re.findall(r'https?://[^\s<>"]+', decoded)
                                if urls:
                                    url = urls[0]
                                    logger.info(f"Base64 ë””ì½”ë”©ì—ì„œ URL ì¶”ì¶œ: {url[:100]}...")
                        except:
                            pass
                except:
                    pass
            
            # ë°©ë²• 3: ê°•í™”ëœ ë¦¬ë‹¤ì´ë ‰íŠ¸ ì¶”ì 
            if 'news.google.com' in url:
                try:
                    # ë‹¤ì–‘í•œ User-Agent ì‹œë„
                    user_agents = [
                        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
                    ]
                    
                    import random
                    for ua in user_agents:
                        try:
                            headers = {
                                'User-Agent': ua,
                                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                                'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
                                'Referer': 'https://www.google.com/'
                            }
                            
                            resp = requests.get(url, headers=headers, timeout=15, allow_redirects=True)
                            if resp.url != url and 'news.google.com' not in resp.url:
                                url = resp.url
                                logger.info(f"ë¦¬ë‹¤ì´ë ‰íŠ¸ë¡œ URL ë°œê²¬: {url[:100]}...")
                                break
                            
                            # HTMLì—ì„œ ì§ì ‘ URL ì°¾ê¸°
                            if 'http' in resp.text:
                                import re
                                # ë” ì •êµí•œ íŒ¨í„´ìœ¼ë¡œ URL ì¶”ì¶œ
                                url_patterns = [
                                    r'"(https?://[^"]+\.co\.kr[^"]*?)"',
                                    r'"(https?://[^"]+\.com[^"]*?)"',
                                    r'href="(https?://[^"]+?)"',
                                    r'url=(https?://[^&\s]+)'
                                ]
                                
                                for pattern in url_patterns:
                                    matches = re.findall(pattern, resp.text)
                                    for match in matches:
                                        if ('news.google.com' not in match and 
                                            'googleusercontent.com' not in match and
                                            'googleapis.com' not in match and
                                            len(match) > 30 and 
                                            any(domain in match for domain in ['.co.kr', '.com', '.net']) and
                                            any(news_site in match for news_site in ['news', 'article', 'www'])):
                                            url = match
                                            logger.info(f"HTMLì—ì„œ URL ì¶”ì¶œ: {url[:100]}...")
                                            break
                                    if url != original_url:
                                        break
                            
                            if url != original_url:
                                break
                                
                        except Exception as e:
                            logger.debug(f"User-Agent {ua[:20]}... ì‹¤íŒ¨: {e}")
                            continue
                            
                except Exception as e:
                    logger.debug(f"ë¦¬ë‹¤ì´ë ‰íŠ¸ ì‹¤íŒ¨: {e}")
        
        # RSS ì„¤ëª…ì„ ê¸°ë³¸ ì½˜í…ì¸ ë¡œ ì‚¬ìš© (ì •ì œ í›„)
        if url == original_url and rss_description:
            # RSS ì„¤ëª…ì—ì„œ ì œëª©ê³¼ ì¶œì²˜ ì œê±°
            clean_desc = rss_description
            # ì œëª© ì œê±°
            if clean_title in clean_desc:
                clean_desc = clean_desc.replace(clean_title, '').strip()
            # ì¶œì²˜ ì œê±°
            import re
            clean_desc = re.sub(r'[ê°€-í£A-Za-z0-9]+\s*$', '', clean_desc).strip()
            clean_desc = re.sub(r'\s*-\s*[ê°€-í£A-Za-z0-9\s]+$', '', clean_desc).strip()
            
            if len(clean_desc) > 30:
                logger.info(f"RSS ì„¤ëª… ì •ì œ í›„ ì‚¬ìš©: {len(clean_desc)}ì")
                return clean_desc[:800], ""
            else:
                logger.info(f"RSS ì„¤ëª…ì´ ë„ˆë¬´ ì§§ìŒ, ì›ë³¸ ì‚¬ìš©: {len(rss_description)}ì")
                return rss_description[:800], ""
        elif url == original_url:
            logger.warning(f"ì‹¤ì œ URL ì¶”ì¶œ ì‹¤íŒ¨, ì›ë³¸ URL ì‚¬ìš©: {url[:100]}...")
        
        # ê°•í™”ëœ í—¤ë”
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Cache-Control': 'no-cache',
            'Referer': 'https://www.google.com/'
        }
        
        resp = requests.get(url, headers=headers, timeout=15)
        resp.encoding = 'utf-8'
        soup = BeautifulSoup(resp.content, "html.parser")
        
        # ê´‘ê³ /ë…¸ì´ì¦ˆ ì œê±°
        for unwanted in soup.select('script, style, nav, header, footer, .ad, .advertisement, .social-share'):
            unwanted.decompose()
        
        # í•œêµ­ ì£¼ìš” ì–¸ë¡ ì‚¬ë³„ íŠ¹í™” ì…€ë ‰í„°
        content_selectors = [
            # í—¬ë¡œë””ë”” íŠ¹í™”
            ".article_txt p", ".article_content p", ".view_txt p",
            # ì£¼ìš” ì¼ê°„ì§€
            ".article_body p", ".news_body p", ".article-body p", ".news-body p",
            ".view_content p", ".article_view p", ".news_view p",
            "#article_body p", "#news_body p", "#articleBody p",
            # ì¸í„°ë„· ë§¤ì²´
            ".content_area p", ".article_content p", ".news_content p",
            ".article_txt p", ".news_txt p", ".txt_area p",
            # ë°©ì†¡ì‚¬
            ".article_wrap p", ".news_wrap p", ".content_wrap p",
            ".view_area p", ".read_body p", ".article_area p",
            # IT/ê³¼í•™ ë§¤ì²´
            ".post_content p", ".entry_content p", ".article_detail p",
            ".content_body p", ".main_content p", ".detail_content p",
            # ì¼ë°˜ì ì¸ ì…€ë ‰í„°
            "article p", ".article-content p", ".post-content p",
            ".entry-content p", ".story-body p", ".article-text p",
            # ë°±ì—… ì…€ë ‰í„°
            "main p", ".main-content p", "[class*='content'] p",
            "[class*='article'] p", "[class*='news'] p", "[class*='body'] p",
            ".container p", ".wrapper p", "section p", "div p"
        ]
        
        content = ""
        for selector in content_selectors:
            paragraphs = soup.select(selector)
            if len(paragraphs) >= 2:  # ìµœì†Œ 2ê°œ ë¬¸ë‹¨ ì´ìƒ
                valid_paragraphs = []
                for p in paragraphs[:8]:  # ìµœëŒ€ 8ê°œ ë¬¸ë‹¨
                    text = p.get_text(strip=True)
                    # ë” ì—„ê²©í•œ í•„í„°ë§
                    if (len(text) > 30 and 
                        not any(skip in text.lower() for skip in [
                            'ê´‘ê³ ', 'êµ¬ë…', 'ë¡œê·¸ì¸', 'íšŒì›ê°€ì…', 'ëŒ“ê¸€', 'ê³µìœ í•˜ê¸°',
                            'ì´ë©”ì¼', 'í˜ì´ìŠ¤ë¶', 'íŠ¸ìœ„í„°', 'ì¹´ì¹´ì˜¤í†¡', 'ë¼ì¸',
                            'copyright', 'â“’', 'Â©', 'ì €ì‘ê¶Œ', 'ë¬´ë‹¨ì „ì¬'
                        ]) and
                        not text.startswith(('ì‚¬ì§„=', 'ì´ë¯¸ì§€=', 'ì¶œì²˜=', 'ê¸°ì='))):
                        valid_paragraphs.append(text)
                
                if len(valid_paragraphs) >= 2:
                    content = "\n\n".join(valid_paragraphs[:5])  # ìµœëŒ€ 5ê°œ ë¬¸ë‹¨
                    logger.info(f"ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ ì„±ê³µ: {len(content)}ì")
                    break
        
        # ë³¸ë¬¸ì´ ì—†ìœ¼ë©´ ë©”íƒ€ ì„¤ëª… ì‚¬ìš©
        if not content or len(content) < 100:
            meta_desc = soup.find('meta', attrs={'name': 'description'}) or soup.find('meta', attrs={'property': 'og:description'})
            if meta_desc and meta_desc.get('content'):
                desc = meta_desc.get('content').strip()
                if len(desc) > 50:
                    content = desc
                    logger.info(f"ë©”íƒ€ ì„¤ëª…ì—ì„œ ë‚´ìš© ì¶”ì¶œ: {len(content)}ì")
        
        # ì´ë¯¸ì§€ ì¶”ì¶œ ê°•í™”
        image_url = ""
        img_selectors = [
            # ë©”íƒ€ íƒœê·¸ ìš°ì„ 
            "meta[property='og:image']", "meta[name='twitter:image']",
            # ê¸°ì‚¬ ë‚´ ì´ë¯¸ì§€
            ".article_body img[src]", ".news_body img[src]", ".article-body img[src]",
            ".view_content img[src]", ".article_view img[src]", ".content img[src]",
            "article img[src]", ".article-content img[src]", "main img[src]"
        ]
        
        for selector in img_selectors:
            if 'meta' in selector:
                meta_img = soup.select_one(selector)
                if meta_img and meta_img.get('content'):
                    src = meta_img.get('content')
                    if src and (src.startswith('http') or src.startswith('/')):
                        if not src.startswith('http'):
                            from urllib.parse import urljoin
                            src = urljoin(url, src)
                        if any(ext in src.lower() for ext in ['.jpg', '.png', '.jpeg', '.webp', '.gif']):
                            image_url = src
                            logger.info(f"ì´ë¯¸ì§€ ë°œê²¬: {src[:100]}...")
                            break
            else:
                img = soup.select_one(selector)
                if img and img.get('src'):
                    src = img.get('src')
                    if src and (src.startswith('http') or src.startswith('/')):
                        if not src.startswith('http'):
                            from urllib.parse import urljoin
                            src = urljoin(url, src)
                        if any(ext in src.lower() for ext in ['.jpg', '.png', '.jpeg', '.webp', '.gif']):
                            image_url = src
                            logger.info(f"ì´ë¯¸ì§€ ë°œê²¬: {src[:100]}...")
                            break
        
        # ì½˜í…ì¸ ê°€ ì—†ê±°ë‚˜ ë„ˆë¬´ ì§§ìœ¼ë©´ RSS ì„¤ëª… ì‚¬ìš©
        if (not content or len(content.strip()) < 100) and rss_description:
            # RSS ì„¤ëª… ì •ì œ
            clean_desc = rss_description
            # ì œëª© ì œê±° (í•¨ìˆ˜ íŒŒë¼ë¯¸í„°ì—ì„œ ë°›ì€ clean_title ì‚¬ìš© ë¶ˆê°€)
            import re
            # ì œëª©ê³¼ ë¹„ìŠ·í•œ ë¶€ë¶„ ì œê±°
            clean_desc = re.sub(r'[ê°€-í£A-Za-z0-9\s\'"\-â€™]+\s*$', '', clean_desc).strip()
            if len(clean_desc) > 50:
                content = clean_desc
                logger.info(f"RSS ì„¤ëª…ì„ ì½˜í…ì¸ ë¡œ ì‚¬ìš©: {len(content)}ì")
            else:
                content = rss_description
                logger.info(f"RSS ì›ë³¸ì„ ì½˜í…ì¸ ë¡œ ì‚¬ìš©: {len(content)}ì")
        
        return content[:1500] if content else "", image_url
        
    except Exception as e:
        logger.error(f"ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨ ({url[:50]}...): {e}")
        return "", ""

def crawl_alternative_sources():
    """ë‹¤ì–‘í•œ ëŒ€ì²´ ë‰´ìŠ¤ ì†ŒìŠ¤ í¬ë¡¤ë§"""
    import random
    articles = []
    
    # ë‹¤ì–‘í•œ ìš°ì£¼ ê´€ë ¨ RSS ì†ŒìŠ¤
    sources = [
        {
            'name': 'ì‚¬ì´ì–¸ìŠ¤íƒ€ì„ì¦ˆ',
            'url': 'https://www.sciencetimes.co.kr/rss/S1N8.xml',
            'keywords': ['ìš°ì£¼', 'ë¡œì¼“', 'ì¸ê³µìœ„ì„±', 'NASA', 'íƒì‚¬']
        },
        {
            'name': 'ì—°í•©ë‰´ìŠ¤',
            'url': 'https://www.yna.co.kr/rss/science.xml',
            'keywords': ['ìš°ì£¼', 'í•­ê³µ', 'ë¡œì¼“', 'ì¸ê³µìœ„ì„±']
        },
        {
            'name': 'ITì¡°ì„ ',
            'url': 'https://rss.itchosun.com/itchosun_news.xml',
            'keywords': ['ìš°ì£¼', 'ìœ„ì„±', 'ë¡œì¼“', 'í•­ê³µ']
        }
    ]
    
    # ëœë¤ìœ¼ë¡œ ì†ŒìŠ¤ ìˆœì„œ ì„®ê¸°
    random.shuffle(sources)
    
    for source in sources:
        try:
            resp = requests.get(source['url'], timeout=10)
            soup = BeautifulSoup(resp.content, "xml")
            
            found_articles = 0
            for item in soup.find_all("item")[:10]:  # ìµœëŒ€ 10ê°œ í™•ì¸
                title = item.find("title").get_text(strip=True) if item.find("title") else ""
                link = item.find("link").get_text(strip=True) if item.find("link") else ""
                desc = item.find("description").get_text(strip=True) if item.find("description") else ""
                
                # ë‹¤ì–‘í•œ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
                if title and any(keyword in title for keyword in source['keywords']):
                    articles.append({
                        'title': title,
                        'content': f"{source['name']}ì—ì„œ ë³´ë„í•œ ìš°ì£¼ ê³¼í•™ ë‰´ìŠ¤ì…ë‹ˆë‹¤.\n\n{desc}\n\nğŸ”— ì›ë¬¸: {link}",
                        'source': source['name']
                    })
                    logger.info(f"{source['name']}: {title[:30]}...")
                    found_articles += 1
                    
                    if found_articles >= 2:  # ì†ŒìŠ¤ë‹¹ ìµœëŒ€ 2ê°œ
                        break
                        
        except Exception as e:
            logger.debug(f"{source['name']} RSS ì‹¤íŒ¨: {e}")
    
    return articles

def get_optimized_space_news():
    """ë‹¤ì–‘í•œ ìš°ì£¼ ë‰´ìŠ¤ ìˆ˜ì§‘ (5ê°œ ë³´ì¥) - ë§¤ë²ˆ ë‹¤ë¥¸ ë‚´ìš©"""
    import random
    all_articles = []
    
    # 1ì°¨: êµ¬ê¸€ ë‰´ìŠ¤ (ìµœëŒ€ 2ê°œ)
    google_articles = crawl_google_news_optimized()
    if google_articles:
        # ëœë¤ìœ¼ë¡œ 1-2ê°œ ì„ íƒ
        selected_google = random.sample(google_articles, min(random.randint(1, 2), len(google_articles)))
        all_articles.extend(selected_google)
    
    # 2ì°¨: ëŒ€ì²´ RSS ì†ŒìŠ¤ (ìµœëŒ€ 2ê°œ)
    alt_articles = crawl_alternative_sources()
    if alt_articles:
        selected_alt = random.sample(alt_articles, min(2, len(alt_articles)))
        all_articles.extend(selected_alt)
    logger.info(f"ëŒ€ì²´ ì†ŒìŠ¤ì—ì„œ {len(alt_articles)}ê°œ ìˆ˜ì§‘")
    
    # 3ì°¨: ë‹¤ì–‘í•œ ìš°ì£¼ ë‰´ìŠ¤ ìƒì„± (í•­ìƒ ì¶”ê°€)
    diverse_articles = generate_diverse_space_news()
    all_articles.extend(diverse_articles)
    logger.info(f"ë‹¤ì–‘í•œ ìš°ì£¼ ë‰´ìŠ¤ {len(diverse_articles)}ê°œ ì¶”ê°€")
    
    # ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ë°˜)
    unique_articles = []
    seen_titles = set()
    
    for article in all_articles:
        title_key = article['title'][:30].lower()  # ì œëª© ì• 30ìë¡œ ë¹„êµ
        if title_key not in seen_titles:
            unique_articles.append(article)
            seen_titles.add(title_key)
    
    # ëœë¤ ì„®ê¸°ë¡œ ë‹¤ì–‘ì„± ë³´ì¥
    random.shuffle(unique_articles)
    
    logger.info(f"ì´ {len(unique_articles)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì„±ê³µ")
    return unique_articles[:5]  # ìµœëŒ€ 5ê°œ

def generate_diverse_space_news():
    """ë‹¤ì–‘í•œ ìš°ì£¼ ë‰´ìŠ¤ ìƒì„± (ë§¤ë²ˆ ë‹¤ë¥¸ ë‚´ìš©)"""
    import random
    from datetime import datetime
    
    # ë” ë‹¤ì–‘í•œ ì£¼ì œë“¤
    topics_pool = [
        # ê¸°ìˆ  ë°œì „
        {
            'title': 'ìš°ì£¼ ê¸°ìˆ  ë°œì „ìœ¼ë¡œ ì¸ë¥˜ ë¯¸ë˜ ë³€í™” ì˜ˆìƒ',
            'content': 'ìµœê·¼ ìš°ì£¼ ê¸°ìˆ ì˜ ê¸‰ì†í•œ ë°œì „ìœ¼ë¡œ ì¸í•´ ì¸ë¥˜ì˜ ë¯¸ë˜ê°€ í¬ê²Œ ë³€í™”í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤. ìš°ì£¼ ì—¬í–‰, ìš°ì£¼ ì •ì°©, ìš°ì£¼ ìì› ì±„êµ´ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í˜ì‹ ì´ ì´ë£¨ì–´ì§€ê³  ìˆìŠµë‹ˆë‹¤.',
            'category': 'ê¸°ìˆ '
        },
        {
            'title': 'ì „ ì„¸ê³„ ìš°ì£¼ ê°œë°œ ê²½ìŸ ì‹¬í™”, í•œêµ­ì˜ ì—­í• ì€?',
            'content': 'ë¯¸êµ­, ì¤‘êµ­, ìœ ëŸ½ ë“± ì£¼ìš” êµ­ê°€ë“¤ì˜ ìš°ì£¼ ê°œë°œ ê²½ìŸì´ ì‹¬í™”ë˜ê³  ìˆëŠ” ê°€ìš´ë°, í•œêµ­ë„ ëˆ„ë¦¬í˜¸ ë¡œì¼“ê³¼ ë‹¬ íƒì‚¬ ê³„íš ë“±ì„ í†µí•´ ìš°ì£¼ ê°•êµ­ìœ¼ë¡œ ë„ì•½í•˜ê³  ìˆìŠµë‹ˆë‹¤.',
            'category': 'êµ­ì œ'
        },
        # ìƒìš©í™”
        {
            'title': 'ìš°ì£¼ ì—¬í–‰ ìƒìš©í™” ì‹œëŒ€, ì¼ë°˜ì¸ë„ ìš°ì£¼ë¡œ',
            'content': 'ë¯¼ê°„ ìš°ì£¼ ê¸°ì—…ë“¤ì˜ ê¸°ìˆ  ë°œì „ìœ¼ë¡œ ìš°ì£¼ ì—¬í–‰ì´ ì ì°¨ í˜„ì‹¤ì´ ë˜ê³  ìˆìŠµë‹ˆë‹¤. ì•ìœ¼ë¡œ 10ë…„ ë‚´ì— ì¼ë°˜ì¸ë„ ìš°ì£¼ ì—¬í–‰ì„ ê²½í—˜í•  ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.',
            'category': 'ìƒìš©í™”'
        },
        # AI/ë¡œë´‡
        {
            'title': 'ì¸ê³µì§€ëŠ¥ê³¼ ë¡œë´‡ì´ ìš°ì£¼ íƒì‚¬ë¥¼ ë³€í™”ì‹œí‚¤ê³  ìˆë‹¤',
            'content': 'AIì™€ ë¡œë´‡ ê¸°ìˆ ì˜ ë°œì „ìœ¼ë¡œ ìš°ì£¼ íƒì‚¬ ë°©ì‹ì´ í˜ì‹ ì ìœ¼ë¡œ ë³€í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤. ììœ¨ íƒì‚¬ ë¡œë´‡ê³¼ AI ê¸°ë°˜ ë°ì´í„° ë¶„ì„ìœ¼ë¡œ ë” íš¨ìœ¨ì ì¸ ìš°ì£¼ íƒì‚¬ê°€ ê°€ëŠ¥í•´ì§€ê³  ìˆìŠµë‹ˆë‹¤.',
            'category': 'AI'
        },
        # ì‚°ì—…
        {
            'title': 'ìš°ì£¼ ì‚°ì—…ì˜ ë¯¸ë˜, ìƒˆë¡œìš´ ì¼ìë¦¬ ì°½ì¶œ ê¸°ëŒ€',
            'content': 'ìš°ì£¼ ì‚°ì—…ì˜ ì„±ì¥ìœ¼ë¡œ ìƒˆë¡œìš´ ì§ì—…ê³¼ ì¼ìë¦¬ê°€ ì°½ì¶œë˜ê³  ìˆìŠµë‹ˆë‹¤. ìš°ì£¼ ì—”ì§€ë‹ˆì–´, ìš°ì£¼ ê´€ê´‘ ê°€ì´ë“œ, ìš°ì£¼ ìì› ì±„êµ´ ì „ë¬¸ê°€ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ìƒˆë¡œìš´ ê¸°íšŒê°€ ì—´ë¦¬ê³  ìˆìŠµë‹ˆë‹¤.',
            'category': 'ì‚°ì—…'
        },
        # ì¶”ê°€ ì£¼ì œë“¤
        {
            'title': 'ë‹¬ ê¸°ì§€ ê±´ì„¤ í”„ë¡œì íŠ¸, ì¸ë¥˜ ìš°ì£¼ ì •ì°©ì˜ ì²« ê±¸ìŒ',
            'content': 'ê°êµ­ì˜ ë‹¬ ê¸°ì§€ ê±´ì„¤ ê³„íšì´ êµ¬ì²´í™”ë˜ê³  ìˆìŠµë‹ˆë‹¤. ë‹¬ ê¸°ì§€ëŠ” ì¸ë¥˜ì˜ ìš°ì£¼ ì •ì°©ì„ ìœ„í•œ ì²« ë²ˆì§¸ ë‹¨ê³„ë¡œ ì—¬ê²¨ì§€ê³  ìˆìŠµë‹ˆë‹¤.',
            'category': 'íƒì‚¬'
        },
        {
            'title': 'ìš°ì£¼ ì“°ë ˆê¸° ë¬¸ì œ ì‹¬ê°, ì²­ì†Œ ê¸°ìˆ  ê°œë°œ ì‹œê¸‰',
            'content': 'ì§€êµ¬ ê¶¤ë„ìƒì˜ ìš°ì£¼ ì“°ë ˆê¸°ê°€ ì‹¬ê°í•œ ë¬¸ì œë¡œ ëŒ€ë‘ë˜ê³  ìˆìŠµë‹ˆë‹¤. ìš°ì£¼ ì“°ë ˆê¸° ì²­ì†Œ ê¸°ìˆ  ê°œë°œì´ ì‹œê¸‰í•œ ê³¼ì œë¡œ ë– ì˜¤ë¥´ê³  ìˆìŠµë‹ˆë‹¤.',
            'category': 'í™˜ê²½'
        },
        {
            'title': 'ì†Œí–‰ì„± ì±„êµ´ ê³„íš, ìš°ì£¼ ìì› í™•ë³´ì˜ ìƒˆë¡œìš´ ê¸¸',
            'content': 'ì†Œí–‰ì„±ì—ì„œ í¬ê·€ ê¸ˆì†ê³¼ ê´‘ë¬¼ì„ ì±„êµ´í•˜ëŠ” ê³„íšì´ í˜„ì‹¤í™”ë˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì§€êµ¬ì˜ ìì› ë¶€ì¡± ë¬¸ì œë¥¼ í•´ê²°í•  ìƒˆë¡œìš´ ë°©ë²•ìœ¼ë¡œ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤.',
            'category': 'ìì›'
        },
        {
            'title': 'ìš°ì£¼ ì •ê±°ì¥ ê±´ì„¤, ì¸ë¥˜ ìš°ì£¼ ì‹œëŒ€ ì—´ë¦°ë‹¤',
            'content': 'ìš°ì£¼ ì •ê±°ì¥ ê±´ì„¤ ê³„íšì´ êµ¬ì²´ì ìœ¼ë¡œ ì§„í–‰ë˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì¸ë¥˜ê°€ ìš°ì£¼ì—ì„œ ì˜êµ¬ì ìœ¼ë¡œ ê±°ì£¼í•  ìˆ˜ ìˆëŠ” ê¸°ë°˜ì„ ë§ˆë ¨í•˜ëŠ” ì¤‘ìš”í•œ ë‹¨ê³„ì…ë‹ˆë‹¤.',
            'category': 'ì •ì°©'
        },
        {
            'title': 'ìš°ì£¼ ë‚ ì”¨ ì˜ˆë³´ ì‹œìŠ¤í…œ, ì§€êµ¬ ê¸°í›„ ë³€í™” ëŒ€ì‘',
            'content': 'ì¸ê³µìœ„ì„±ì„ í™œìš©í•œ ìš°ì£¼ ë‚ ì”¨ ì˜ˆë³´ ì‹œìŠ¤í…œì´ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì§€êµ¬ ê¸°í›„ ë³€í™”ì— ë” íš¨ê³¼ì ìœ¼ë¡œ ëŒ€ì‘í•  ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤.',
            'category': 'ê¸°í›„'
        }
    ]
    
    # ëœë¤ìœ¼ë¡œ 2-4ê°œ ì„ íƒ (ë‹¤ì–‘ì„± ë³´ì¥)
    selected_count = random.randint(2, 4)
    selected_topics = random.sample(topics_pool, selected_count)
    
    result = []
    for i, topic in enumerate(selected_topics):
        # ë‹¤ì–‘í•œ ì†ŒìŠ¤ ì´ë¦„ ì‚¬ìš©
        sources = ['SpaceNews', 'KoreaSpace', 'ScienceDaily', 'SpaceTech', 'AstroNews']
        source_name = random.choice(sources)
        
        result.append({
            'title': topic['title'],
            'content': f"{source_name}ì—ì„œ ë³´ë„í•œ ìš°ì£¼ ê³¼í•™ ë‰´ìŠ¤ì…ë‹ˆë‹¤.\n\n{topic['content']}\n\nğŸ·ï¸ ë¶„ë¥˜: {topic['category']}\nğŸ”— ìì„¸í•œ ë‚´ìš©ì€ ê´€ë ¨ ìš°ì£¼ ê¸°ê´€ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            'source': source_name
        })
    
    return result