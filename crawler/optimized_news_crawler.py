#!/usr/bin/env python3
"""
ìµœì í™”ëœ ìš°ì£¼ ë‰´ìŠ¤ í¬ë¡¤ë§ (ë‚ ì§œ í•„í„°ë§ í¬í•¨)
"""
import requests
from bs4 import BeautifulSoup
import logging
from datetime import datetime, timedelta
from dateutil import parser
import re

logger = logging.getLogger(__name__)

def is_recent_news(pub_date_str, max_days=7):
    """ë‰´ìŠ¤ê°€ ìµœê·¼ Nì¼ ì´ë‚´ì¸ì§€ í™•ì¸"""
    try:
        if not pub_date_str:
            return True  # ë‚ ì§œ ì •ë³´ê°€ ì—†ìœ¼ë©´ í¬í•¨
        
        # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ íŒŒì‹±
        pub_date = parser.parse(pub_date_str)
        cutoff_date = datetime.now() - timedelta(days=max_days)
        
        return pub_date.replace(tzinfo=None) >= cutoff_date
    except:
        return True  # íŒŒì‹± ì‹¤íŒ¨ì‹œ í¬í•¨

def is_valid_content(title, content):
    """ìœ íš¨í•œ ë‰´ìŠ¤ ì½˜í…ì¸ ì¸ì§€ í™•ì¸"""
    if not title or len(title.strip()) < 10:
        return False
    if content and len(content.strip()) < 20:
        return False
    return True

def crawl_google_news_optimized():
    """ìµœì í™”ëœ êµ¬ê¸€ ë‰´ìŠ¤ í¬ë¡¤ë§"""
    try:
        import random
        import time
        
        # ìºì‹œ ë°©ì§€ë¥¼ ìœ„í•œ ëœë¤ íŒŒë¼ë¯¸í„° ì¶”ê°€
        cache_buster = int(time.time()) + random.randint(1, 1000)
        url = f"https://news.google.com/rss/search?q=ìš°ì£¼+ë‰´ìŠ¤&hl=ko&gl=KR&ceid=KR:ko&_cb={cache_buster}"
        headers = {
            'User-Agent': f'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/{random.randint(100,120)}.0.0.0 Safari/537.36',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache'
        }
        
        resp = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(resp.content, "xml")
        
        articles = []
        seen_titles = set()  # ì¤‘ë³µ ì œê±°ìš©
        
        for item in soup.find_all("item")[:10]:
            title_tag = item.find("title")
            link_tag = item.find("link")
            source_tag = item.find("source")
            pub_date_tag = item.find("pubDate")
            
            if not title_tag:
                continue
                
            title = title_tag.get_text(strip=True)
            pub_date = pub_date_tag.get_text(strip=True) if pub_date_tag else ""
            
            # ë‚ ì§œ í•„í„°ë§ (ìµœê·¼ 7ì¼)
            if not is_recent_news(pub_date, max_days=7):
                logger.debug(f"ì˜¤ë˜ëœ ë‰´ìŠ¤ ì œì™¸: {title[:30]}...")
                continue
            
            # ì œëª©ì—ì„œ ì¶œì²˜ ì™„ì „ ì œê±° (ì˜ˆ: [í•œêµ­ëŒ€í•™ì‹ ë¬¸], - í•œêµ­ëŒ€í•™ì‹ ë¬¸ ë“±)
            clean_title = re.sub(r'^\[.*?\]\s*', '', title).strip()
            clean_title = re.sub(r'\s*-\s*[ê°€-í£A-Za-z0-9\s]+$', '', clean_title).strip()
            
            # ì¤‘ë³µ ì œê±°
            if clean_title in seen_titles:
                continue
            seen_titles.add(clean_title)
            
            link = link_tag.get_text(strip=True) if link_tag else ""
            source = source_tag.get_text(strip=True) if source_tag else "êµ¬ê¸€ë‰´ìŠ¤"
            
            # ìƒì„¸ ë‚´ìš© ì¶”ì¶œ
            content, image_url = get_article_content(link)
            
            # ë¬´ì˜ë¯¸í•œ ì½˜í…ì¸  í•„í„°ë§
            if content and any(skip_text in content for skip_text in [
                'Google ë‰´ìŠ¤ê°€ ì „ì„¸ê³„', 'ì „ì„¸ê³„ ë§¤ì²´ë¡œë¶€í„°', 
                'ì¢…í•©í•œ ìµœì‹  ë‰´ìŠ¤', 'ë‰´ìŠ¤ ì†ŒìŠ¤', 'ë‰´ìŠ¤ ì œê³µì—…ì²´'
            ]):
                content = ""  # ë¬´ì˜ë¯¸í•œ ì½˜í…ì¸  ì œê±°
            
            # ì½˜í…ì¸  ìœ íš¨ì„± ê²€ì‚¬ (ë„ˆë¬´ ì—„ê²©í•˜ì§€ ì•Šê²Œ)
            if not clean_title or len(clean_title.strip()) < 5:
                logger.debug(f"ìœ íš¨í•˜ì§€ ì•Šì€ ì œëª© ì œì™¸: {title[:30]}...")
                continue
            
            # AI í‰ê°€ ë° ìš”ì•½
            from ai.news_evaluator import evaluate_news_article
            evaluation = evaluate_news_article(clean_title, content, link)
            
            if evaluation["evaluation"] == "REJECT":
                logger.debug(f"AI í‰ê°€ ê±°ë¶€: {title[:30]}... - {evaluation.get('reason', '')}")
                continue
            
            # í’ë¶€í•œ ì½˜í…ì¸  ìƒì„±
            full_content = f"{source}ì—ì„œ ë³´ë„í•œ ìš°ì£¼ ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤ì…ë‹ˆë‹¤.\n\n"
            
            # AI ìš”ì•½ ì¶”ê°€
            if evaluation.get("summary") and len(evaluation["summary"]) > 20:
                full_content += f"ğŸ¤– AI ìš”ì•½: {evaluation['summary']}\n\n"
            
            # ì‹¤ì œ ê¸°ì‚¬ ë‚´ìš© ì¶”ê°€ (ë” ìì„¸íˆ)
            if content and len(content.strip()) > 100:
                # ì „ì²´ ë‚´ìš©ì„ ì ì ˆíˆ ìš”ì•½í•´ì„œ í‘œì‹œ
                content_preview = content[:500] + "..." if len(content) > 500 else content
                full_content += f"ğŸ“° ê¸°ì‚¬ ë‚´ìš©:\n{content_preview}\n\n"
            elif content and len(content.strip()) > 50:
                full_content += f"ğŸ“° ê¸°ì‚¬ ìš”ì•½: {content}\n\n"
            
            # ì´ë¯¸ì§€ ì¶”ê°€
            if image_url:
                full_content += f"ğŸ–¼ï¸ ê´€ë ¨ ì´ë¯¸ì§€: {image_url}\n\n"
            
            if pub_date:
                full_content += f"ğŸ“… ë°œí–‰ì¼: {pub_date}\n"
            
            # AI í‚¤ì›Œë“œ ì¶”ê°€
            if evaluation.get("keywords") and len(evaluation["keywords"]) > 0:
                keywords_str = ", ".join(evaluation["keywords"][:3])
                full_content += f"ğŸ·ï¸ í•µì‹¬ í‚¤ì›Œë“œ: {keywords_str}\n"
            
            full_content += f"ğŸ”— ì›ë¬¸ ë§í¬: {link}\nğŸŒŒ ì¶œì²˜: {source}"
            
            articles.append({
                "title": clean_title,
                "content": full_content,
                "source": "GoogleNews",
                "published_at": pub_date,
                "url": link,
                "ai_evaluation": evaluation
            })
            
            if len(articles) >= 5:  # ìµœëŒ€ 5ê°œ
                break
        
        logger.info(f"êµ¬ê¸€ ë‰´ìŠ¤ ìµœì‹  ìš°ì£¼ ë‰´ìŠ¤ {len(articles)}ê°œ ìˆ˜ì§‘")
        return articles
        
    except Exception as e:
        logger.error(f"êµ¬ê¸€ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

def get_article_content(url):
    """ê¸°ì‚¬ URLì—ì„œ ìƒì„¸ ë‚´ìš© ì¶”ì¶œ (ê°•í™”ë²„ì „)"""
    try:
        import time
        
        # êµ¬ê¸€ ë‰´ìŠ¤ ë¦¬ë‹¤ì´ë ‰íŠ¸ ì²˜ë¦¬ ê°•í™”
        if 'news.google.com' in url:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
                'Referer': 'https://news.google.com/'
            }
            
            # ë‹¨ê³„ë³„ ë¦¬ë‹¤ì´ë ‰íŠ¸ ì¶”ì 
            for attempt in range(3):
                try:
                    resp = requests.get(url, headers=headers, timeout=15, allow_redirects=True)
                    if resp.url != url and 'news.google.com' not in resp.url:
                        url = resp.url
                        logger.info(f"ì‹¤ì œ ê¸°ì‚¬ URL ë°œê²¬: {url[:100]}...")
                        break
                    time.sleep(1)
                except:
                    continue
        
        # ê°•í™”ëœ í—¤ë”
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Cache-Control': 'no-cache'
        }
        
        resp = requests.get(url, headers=headers, timeout=15)
        resp.encoding = 'utf-8'
        soup = BeautifulSoup(resp.content, "html.parser")
        
        # ê´‘ê³ /ë…¸ì´ì¦ˆ ì œê±°
        for unwanted in soup.select('script, style, nav, header, footer, .ad, .advertisement, .social-share'):
            unwanted.decompose()
        
        # í•œêµ­ ì–¸ë¡ ì‚¬ íŠ¹í™” ì…€ë ‰í„° (ìš°ì„ ìˆœìœ„)
        content_selectors = [
            # ì£¼ìš” í•œêµ­ ì–¸ë¡ ì‚¬
            ".article_body p", ".news_body p", ".article-body p", ".news-body p",
            ".view_content p", ".article_view p", ".news_view p",
            "#article_body p", "#news_body p", "#content p",
            ".content_area p", ".article_content p", ".news_content p",
            # ì¼ë°˜ì ì¸ ì…€ë ‰í„°
            "article p", ".article-content p", ".post-content p",
            ".entry-content p", ".story-body p", ".article-text p",
            # ë°±ì—… ì…€ë ‰í„°
            "main p", ".main-content p", "[class*='content'] p",
            "div p", "section p"
        ]
        
        content = ""
        for selector in content_selectors:
            paragraphs = soup.select(selector)
            if len(paragraphs) >= 2:  # ìµœì†Œ 2ê°œ ë¬¸ë‹¨ ì´ìƒ
                valid_paragraphs = []
                for p in paragraphs[:8]:  # ìµœëŒ€ 8ê°œ ë¬¸ë‹¨
                    text = p.get_text(strip=True)
                    # ë” ì—„ê²©í•œ í•„í„°ë§
                    if (len(text) > 30 and 
                        not any(skip in text.lower() for skip in [
                            'ê´‘ê³ ', 'êµ¬ë…', 'ë¡œê·¸ì¸', 'íšŒì›ê°€ì…', 'ëŒ“ê¸€', 'ê³µìœ í•˜ê¸°',
                            'ì´ë©”ì¼', 'í˜ì´ìŠ¤ë¶', 'íŠ¸ìœ„í„°', 'ì¹´ì¹´ì˜¤í†¡', 'ë¼ì¸',
                            'copyright', 'â“’', 'Â©', 'ì €ì‘ê¶Œ', 'ë¬´ë‹¨ì „ì¬'
                        ]) and
                        not text.startswith(('ì‚¬ì§„=', 'ì´ë¯¸ì§€=', 'ì¶œì²˜=', 'ê¸°ì='))):
                        valid_paragraphs.append(text)
                
                if len(valid_paragraphs) >= 2:
                    content = "\n\n".join(valid_paragraphs[:5])  # ìµœëŒ€ 5ê°œ ë¬¸ë‹¨
                    logger.info(f"ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ ì„±ê³µ: {len(content)}ì")
                    break
        
        # ë³¸ë¬¸ì´ ì—†ìœ¼ë©´ ë©”íƒ€ ì„¤ëª… ì‚¬ìš©
        if not content or len(content) < 100:
            meta_desc = soup.find('meta', attrs={'name': 'description'}) or soup.find('meta', attrs={'property': 'og:description'})
            if meta_desc and meta_desc.get('content'):
                desc = meta_desc.get('content').strip()
                if len(desc) > 50:
                    content = desc
                    logger.info(f"ë©”íƒ€ ì„¤ëª…ì—ì„œ ë‚´ìš© ì¶”ì¶œ: {len(content)}ì")
        
        # ì´ë¯¸ì§€ ì¶”ì¶œ ê°•í™”
        image_url = ""
        img_selectors = [
            # ë©”íƒ€ íƒœê·¸ ìš°ì„ 
            "meta[property='og:image']", "meta[name='twitter:image']",
            # ê¸°ì‚¬ ë‚´ ì´ë¯¸ì§€
            ".article_body img[src]", ".news_body img[src]", ".article-body img[src]",
            ".view_content img[src]", ".article_view img[src]", ".content img[src]",
            "article img[src]", ".article-content img[src]", "main img[src]"
        ]
        
        for selector in img_selectors:
            if 'meta' in selector:
                meta_img = soup.select_one(selector)
                if meta_img and meta_img.get('content'):
                    src = meta_img.get('content')
                    if src and (src.startswith('http') or src.startswith('/')):
                        if not src.startswith('http'):
                            from urllib.parse import urljoin
                            src = urljoin(url, src)
                        if any(ext in src.lower() for ext in ['.jpg', '.png', '.jpeg', '.webp', '.gif']):
                            image_url = src
                            logger.info(f"ì´ë¯¸ì§€ ë°œê²¬: {src[:100]}...")
                            break
            else:
                img = soup.select_one(selector)
                if img and img.get('src'):
                    src = img.get('src')
                    if src and (src.startswith('http') or src.startswith('/')):
                        if not src.startswith('http'):
                            from urllib.parse import urljoin
                            src = urljoin(url, src)
                        if any(ext in src.lower() for ext in ['.jpg', '.png', '.jpeg', '.webp', '.gif']):
                            image_url = src
                            logger.info(f"ì´ë¯¸ì§€ ë°œê²¬: {src[:100]}...")
                            break
        
        return content[:1500] if content else "", image_url
        
    except Exception as e:
        logger.error(f"ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨ ({url[:50]}...): {e}")
        return "", ""

def get_optimized_space_news():
    """ìµœì í™”ëœ ìš°ì£¼ ë‰´ìŠ¤ ìˆ˜ì§‘"""
    # êµ¬ê¸€ ë‰´ìŠ¤ ìš°ì„  ì‹œë„
    articles = crawl_google_news_optimized()
    
    if articles:
        logger.info(f"êµ¬ê¸€ ë‰´ìŠ¤ì—ì„œ {len(articles)}ê°œ ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘ ì„±ê³µ")
        return articles
    
    # ì‹¤íŒ¨ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    logger.warning("ëª¨ë“  ë‰´ìŠ¤ ì†ŒìŠ¤ ì‹¤íŒ¨ - ì‹¤ì œ ë°ì´í„° ì—†ìŒ")
    return []