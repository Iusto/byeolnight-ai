"""
RSS í”¼ë“œë¥¼ í†µí•œ ì‹¤ì œ ìš°ì£¼ ë‰´ìŠ¤ í¬ë¡¤ë§
"""
import feedparser
import requests
import logging
from datetime import datetime, timedelta
from typing import List, Dict

logger = logging.getLogger(__name__)

def crawl_nasa_rss():
    """NASA RSS í”¼ë“œì—ì„œ ìš°ì£¼ ë‰´ìŠ¤ í¬ë¡¤ë§"""
    try:
        url = "https://www.nasa.gov/rss/dyn/breaking_news.rss"
        feed = feedparser.parse(url)
        
        articles = []
        for entry in feed.entries[:3]:  # ìµœì‹  3ê°œë§Œ
            title = entry.title
            content = entry.summary if hasattr(entry, 'summary') else entry.description
            link = entry.link
            
            # í•œêµ­ì–´ ë²ˆì—­ (ê°„ë‹¨í•œ ì„¤ëª… ì¶”ê°€)
            korean_content = f"NASAì—ì„œ ë°œí‘œí•œ ìš°ì£¼ ê´€ë ¨ ë‰´ìŠ¤ì…ë‹ˆë‹¤.\n\nì›ì œ: {title}\n\n{content[:200]}...\n\nğŸ”— ì›ë¬¸ ë³´ê¸°: {link}"
            
            articles.append({
                "title": f"[NASA] {title}",
                "content": korean_content,
                "source": "NASA_RSS"
            })
            
        logger.info(f"NASA RSS ë‰´ìŠ¤ {len(articles)}ê°œ ì²˜ë¦¬ ì™„ë£Œ")
        return articles
        
    except Exception as e:
        logger.error(f"NASA RSS í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

def crawl_esa_rss():
    """ESA(ìœ ëŸ½ìš°ì£¼êµ­) RSS í”¼ë“œì—ì„œ ìš°ì£¼ ë‰´ìŠ¤ í¬ë¡¤ë§"""
    try:
        url = "https://www.esa.int/rssfeed/Our_Activities/Space_Science"
        feed = feedparser.parse(url)
        
        articles = []
        for entry in feed.entries[:2]:  # ìµœì‹  2ê°œë§Œ
            title = entry.title
            content = entry.summary if hasattr(entry, 'summary') else entry.description
            link = entry.link
            
            korean_content = f"ìœ ëŸ½ìš°ì£¼êµ­(ESA)ì—ì„œ ë°œí‘œí•œ ìš°ì£¼ê³¼í•™ ë‰´ìŠ¤ì…ë‹ˆë‹¤.\n\nì›ì œ: {title}\n\n{content[:200]}...\n\nğŸ”— ì›ë¬¸ ë³´ê¸°: {link}"
            
            articles.append({
                "title": f"[ESA] {title}",
                "content": korean_content,
                "source": "ESA_RSS"
            })
            
        logger.info(f"ESA RSS ë‰´ìŠ¤ {len(articles)}ê°œ ì²˜ë¦¬ ì™„ë£Œ")
        return articles
        
    except Exception as e:
        logger.error(f"ESA RSS í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

def crawl_spacenews_rss():
    """SpaceNews RSS í”¼ë“œì—ì„œ ìš°ì£¼ ë‰´ìŠ¤ í¬ë¡¤ë§"""
    try:
        url = "https://spacenews.com/feed/"
        feed = feedparser.parse(url)
        
        articles = []
        for entry in feed.entries[:2]:  # ìµœì‹  2ê°œë§Œ
            title = entry.title
            content = entry.summary if hasattr(entry, 'summary') else entry.description
            link = entry.link
            
            korean_content = f"SpaceNewsì—ì„œ ë³´ë„í•œ ìš°ì£¼ì‚°ì—… ë‰´ìŠ¤ì…ë‹ˆë‹¤.\n\nì›ì œ: {title}\n\n{content[:200]}...\n\nğŸ”— ì›ë¬¸ ë³´ê¸°: {link}"
            
            articles.append({
                "title": f"[SpaceNews] {title}",
                "content": korean_content,
                "source": "SpaceNews_RSS"
            })
            
        logger.info(f"SpaceNews RSS ë‰´ìŠ¤ {len(articles)}ê°œ ì²˜ë¦¬ ì™„ë£Œ")
        return articles
        
    except Exception as e:
        logger.error(f"SpaceNews RSS í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

def get_all_rss_news() -> List[Dict]:
    """ëª¨ë“  RSS ì†ŒìŠ¤ì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
    all_articles = []
    
    # NASA RSS
    all_articles.extend(crawl_nasa_rss())
    
    # ESA RSS  
    all_articles.extend(crawl_esa_rss())
    
    # SpaceNews RSS
    all_articles.extend(crawl_spacenews_rss())
    
    logger.info(f"RSS ë‰´ìŠ¤ ì´ {len(all_articles)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
    return all_articles