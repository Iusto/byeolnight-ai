"""
í•œêµ­ì¸ì´ í¥ë¯¸ë¡œì›Œí•  ìš°ì£¼ ë‰´ìŠ¤ í¬ë¡¤ë§
"""
import requests
from bs4 import BeautifulSoup
import logging
from datetime import datetime

logger = logging.getLogger(__name__)

def crawl_ytn_science():
    """YTN ì‚¬ì´ì–¸ìŠ¤ - ìš°ì£¼/í•­ê³µ ë‰´ìŠ¤"""
    try:
        url = "https://science.ytn.co.kr/program/program_view.php?s_mcd=0082&s_hcd="
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        resp = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(resp.content, "html.parser")
        
        articles = []
        for item in soup.select(".list_cont li")[:3]:
            title_tag = item.select_one("a")
            if title_tag and any(word in title_tag.text for word in ['ìš°ì£¼', 'ë¡œì¼“', 'ìœ„ì„±', 'ë‹¬', 'í™”ì„±', 'ì²œë¬¸']):
                title = title_tag.get_text(strip=True)
                link = "https://science.ytn.co.kr" + title_tag.get('href')
                
                articles.append({
                    "title": f"[YTNì‚¬ì´ì–¸ìŠ¤] {title}",
                    "content": f"YTN ì‚¬ì´ì–¸ìŠ¤ì—ì„œ ë³´ë„í•œ ìš°ì£¼ ê´€ë ¨ ë‰´ìŠ¤ì…ë‹ˆë‹¤.\n\n{title}\n\nğŸ”— ì›ë¬¸: {link}",
                    "source": "YTN_Science"
                })
        
        logger.info(f"YTN ì‚¬ì´ì–¸ìŠ¤ ë‰´ìŠ¤ {len(articles)}ê°œ ìˆ˜ì§‘")
        return articles
    except Exception as e:
        logger.error(f"YTN ì‚¬ì´ì–¸ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

def crawl_chosun_science():
    """ì¡°ì„ ì¼ë³´ ì‚¬ì´ì–¸ìŠ¤ - ìš°ì£¼ ë‰´ìŠ¤"""
    try:
        url = "https://www.chosun.com/science-health/"
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        resp = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(resp.content, "html.parser")
        
        articles = []
        for item in soup.select("article, .story-card")[:5]:
            title_tag = item.select_one("h3 a, .headline a, a")
            if title_tag and any(word in title_tag.text for word in ['ìš°ì£¼', 'ë¡œì¼“', 'ìœ„ì„±', 'ë‹¬', 'í™”ì„±', 'ì²œë¬¸', 'NASA', 'ìŠ¤í˜ì´ìŠ¤X']):
                title = title_tag.get_text(strip=True)
                link = title_tag.get('href')
                if not link.startswith('http'):
                    link = "https://www.chosun.com" + link
                
                articles.append({
                    "title": f"[ì¡°ì„ ì¼ë³´] {title}",
                    "content": f"ì¡°ì„ ì¼ë³´ì—ì„œ ë³´ë„í•œ ìš°ì£¼ ê´€ë ¨ ë‰´ìŠ¤ì…ë‹ˆë‹¤.\n\n{title}\n\nğŸ”— ì›ë¬¸: {link}",
                    "source": "Chosun_Science"
                })
        
        logger.info(f"ì¡°ì„ ì¼ë³´ ì‚¬ì´ì–¸ìŠ¤ ë‰´ìŠ¤ {len(articles)}ê°œ ìˆ˜ì§‘")
        return articles
    except Exception as e:
        logger.error(f"ì¡°ì„ ì¼ë³´ ì‚¬ì´ì–¸ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

def crawl_hani_science():
    """í•œê²¨ë ˆ ì‚¬ì´ì–¸ìŠ¤ì˜¨ - ìš°ì£¼ ë‰´ìŠ¤"""
    try:
        url = "http://scienceon.hani.co.kr/category/astronomy"
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        resp = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(resp.content, "html.parser")
        
        articles = []
        for item in soup.select(".article-list li, .post")[:3]:
            title_tag = item.select_one("h3 a, .title a, a")
            if title_tag:
                title = title_tag.get_text(strip=True)
                link = title_tag.get('href')
                if not link.startswith('http'):
                    link = "http://scienceon.hani.co.kr" + link
                
                articles.append({
                    "title": f"[ì‚¬ì´ì–¸ìŠ¤ì˜¨] {title}",
                    "content": f"í•œê²¨ë ˆ ì‚¬ì´ì–¸ìŠ¤ì˜¨ì—ì„œ ë³´ë„í•œ ì²œë¬¸í•™ ë‰´ìŠ¤ì…ë‹ˆë‹¤.\n\n{title}\n\nğŸ”— ì›ë¬¸: {link}",
                    "source": "ScienceOn"
                })
        
        logger.info(f"ì‚¬ì´ì–¸ìŠ¤ì˜¨ ë‰´ìŠ¤ {len(articles)}ê°œ ìˆ˜ì§‘")
        return articles
    except Exception as e:
        logger.error(f"ì‚¬ì´ì–¸ìŠ¤ì˜¨ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

def get_korean_space_news():
    """í•œêµ­ì¸ ë§ì¶¤ ìš°ì£¼ ë‰´ìŠ¤ ìˆ˜ì§‘"""
    all_articles = []
    
    all_articles.extend(crawl_ytn_science())
    all_articles.extend(crawl_chosun_science()) 
    all_articles.extend(crawl_hani_science())
    
    logger.info(f"í•œêµ­ ìš°ì£¼ ë‰´ìŠ¤ ì´ {len(all_articles)}ê°œ ìˆ˜ì§‘")
    return all_articles