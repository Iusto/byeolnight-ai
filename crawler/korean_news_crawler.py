import requests
from bs4 import BeautifulSoup
import logging
from datetime import datetime
import urllib3
from crawler.content_extractor import extract_article_content, extract_kasi_article, extract_sciencetimes_article
from crawler.content_formatter import format_news_article
from translator.free_translator import smart_free_translate
from crawler.all_observatory_crawler import crawl_all_korea_observatories

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
logger = logging.getLogger(__name__)

def crawl_sciencetimes():
    """ì‚¬ì´ì–¸ìŠ¤íƒ€ì„ì¦ˆ - ëŒ€ì¤‘ ì¹œí™”ì  ìš°ì£¼ê³¼í•™ ë‰´ìŠ¤"""
    try:
        url = "https://www.sciencetimes.co.kr/news/category/%ec%9a%b0%ec%a3%bc%eb%b3%91%ea%b3%84"
        resp = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
        soup = BeautifulSoup(resp.content, "html.parser")

        articles = []
        for item in soup.select("ul.article-list li")[:3]:
            title_tag = item.select_one("h4 a")
            if title_tag:
                title = title_tag.get_text(strip=True)
                article_url = title_tag.get('href')
                
                # ì‹¤ì œ ê¸°ì‚¬ ë‚´ìš©ê³¼ ì´ë¯¸ì§€ ì¶”ì¶œ
                full_content, image_url = extract_sciencetimes_article(article_url)
                
                # ê¸°ì‚¬ í¬ë§·íŒ…
                formatted_title, formatted_content = format_news_article(
                    title, full_content, "ì‚¬ì´ì–¸ìŠ¤íƒ€ì„ì¦ˆ", article_url, image_url
                )
                
                articles.append({
                    "title": formatted_title,
                    "content": formatted_content,
                    "source": "ScienceTimes"
                })
                logger.info(f"ì‚¬ì´ì–¸ìŠ¤íƒ€ì„ì¦ˆ ê¸°ì‚¬ ì²˜ë¦¬ ì™„ë£Œ: {title[:50]}...")
        return articles
    except Exception as e:
        logger.error(f"ì‚¬ì´ì–¸ìŠ¤íƒ€ì„ì¦ˆ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

def crawl_kasi():
    """í•œêµ­ì²œë¬¸ì—°êµ¬ì› - ê³µì‹ ë°œí‘œ ë° ê¸°ìˆ  ìë£Œ"""
    try:
        url = "https://www.kasi.re.kr/kor/publication/pressRelease"
        resp = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, verify=False, timeout=10)
        soup = BeautifulSoup(resp.content, "html.parser")
        
        articles = []
        for item in soup.select(".board_list tbody tr")[:3]:
            title_tag = item.select_one("td.subject a")
            if title_tag:
                title = title_tag.get_text(strip=True)
                article_url = title_tag.get('href')
                full_url = f"https://www.kasi.re.kr{article_url}"
                
                # ì‹¤ì œ ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ
                full_content, image_url = extract_kasi_article(article_url)
                
                # ê¸°ì‚¬ í¬ë§·íŒ…
                formatted_title, formatted_content = format_news_article(
                    title, full_content, "í•œêµ­ì²œë¬¸ì—°êµ¬ì›", full_url, image_url
                )
                
                articles.append({
                    "title": formatted_title,
                    "content": formatted_content,
                    "source": "KASI"
                })
                logger.info(f"KASI ê¸°ì‚¬ ì²˜ë¦¬ ì™„ë£Œ: {title[:50]}...")
        return articles
    except Exception as e:
        logger.error(f"KASI í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

def crawl_dongascience():
    """ë™ì•„ì‚¬ì´ì–¸ìŠ¤ - ìš°ì£¼/ì²œë¬¸ ì„¹ì…˜"""
    try:
        url = "https://www.dongascience.com/news.php?idx=45"  # ìš°ì£¼/ì²œë¬¸ ì¹´í…Œê³ ë¦¬
        resp = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
        soup = BeautifulSoup(resp.content, "html.parser")
        
        articles = []
        for item in soup.select(".article_list .list_item")[:3]:
            title_tag = item.select_one(".tit a")
            if title_tag:
                title = title_tag.get_text(strip=True)
                article_url = "https://www.dongascience.com" + title_tag.get('href')
                
                # ì‹¤ì œ ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ
                full_content, image_url = extract_article_content(article_url)
                
                # ê¸°ì‚¬ í¬ë§·íŒ…
                formatted_title, formatted_content = format_news_article(
                    title, full_content, "ë™ì•„ì‚¬ì´ì–¸ìŠ¤", article_url, image_url
                )
                
                articles.append({
                    "title": formatted_title,
                    "content": formatted_content,
                    "source": "DongaScience"
                })
                logger.info(f"ë™ì•„ì‚¬ì´ì–¸ìŠ¤ ê¸°ì‚¬ ì²˜ë¦¬ ì™„ë£Œ: {title[:50]}...")
        return articles
    except Exception as e:
        logger.error(f"ë™ì•„ì‚¬ì´ì–¸ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

def crawl_science_museums():
    """ê³¼í•™ê´€ ë° ì²œë¬¸ëŒ€ ì´ë²¤íŠ¸ ì •ë³´"""
    try:
        events = []
        
        # êµ­ë¦½ê³¼ì²œê³¼í•™ê´€ í”„ë¡œê·¸ë¨
        gwacheon_events = [
            {
                "name": "êµ­ë¦½ê³¼ì²œê³¼í•™ê´€",
                "title": "ì²œì²´ê´€ì¸¡êµì‹¤ - ê²¨ìš¸ì²  ë³„ìë¦¬ ê´€ì¸¡",
                "date": "ë§¤ì£¼ í† ìš”ì¼ 19:00-21:00",
                "location": "ê²½ê¸° ê³¼ì²œì‹œ",
                "content": "ê²¨ìš¸ì²  ëŒ€í‘œ ë³„ìë¦¬ì™€ í–‰ì„±ì„ ë§ì›ê²½ìœ¼ë¡œ ì§ì ‘ ê´€ì¸¡í•˜ëŠ” í”„ë¡œê·¸ë¨ì…ë‹ˆë‹¤. ê°€ì¡± ë‹¨ìœ„ ì°¸ì—¬ ê°€ëŠ¥í•˜ë©°, ë‚ ì”¨ì— ë”°ë¼ ì‹¤ë‚´ ì²œë¬¸í•™ ê°•ì˜ë¡œ ëŒ€ì²´ë©ë‹ˆë‹¤.",
                "url": "https://www.sciencecenter.go.kr"
            },
            {
                "name": "êµ­ë¦½ê³¼ì²œê³¼í•™ê´€",
                "title": "ê°€ì¡±ì²œë¬¸êµì‹¤ - ìš°ì£¼íƒì‚¬ ì´ì•¼ê¸°",
                "date": "ë§¤ì›” ë‘˜ì§¸, ë„·ì§¸ ì¼ìš”ì¼ 14:00-16:00",
                "location": "ê²½ê¸° ê³¼ì²œì‹œ",
                "content": "ìµœì‹  ìš°ì£¼íƒì‚¬ ì†Œì‹ê³¼ í•¨ê»˜ ê°„ë‹¨í•œ ë§ì›ê²½ ë§Œë“¤ê¸° ì²´í—˜ì„ ì§„í–‰í•©ë‹ˆë‹¤. ì´ˆë“±í•™ìƒ ì´ìƒ ì°¸ì—¬ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
                "url": "https://www.sciencecenter.go.kr"
            }
        ]
        
        # ì§€ì—­ ì²œë¬¸ëŒ€ íŠ¹ë³„ í”„ë¡œê·¸ë¨
        regional_events = [
            {
                "name": "ì˜ì›”ë³„ë§ˆë¡œì²œë¬¸ëŒ€",
                "title": "ê²¨ìš¸ ë³„ì¶•ì œ - ì˜¤ë¦¬ì˜¨ìë¦¬ ëŒ€íƒí—˜",
                "date": "12ì›”-2ì›” ë§¤ì£¼ ê¸ˆ,í† ìš”ì¼",
                "location": "ê°•ì› ì˜ì›”êµ°",
                "content": "ê²¨ìš¸ì²  ê°€ì¥ ì•„ë¦„ë‹¤ìš´ ì˜¤ë¦¬ì˜¨ìë¦¬ì™€ ì£¼ë³€ ì²œì²´ë“¤ì„ ëŒ€í˜• ë§ì›ê²½ìœ¼ë¡œ ê´€ì¸¡í•©ë‹ˆë‹¤. ë³„ë¹› ì‚¬ì§„ ì´¬ì˜ ì²´í—˜ë„ í•¨ê»˜ ì§„í–‰ë©ë‹ˆë‹¤.",
                "url": "http://www.yao.or.kr"
            },
            {
                "name": "ë³´í˜„ì‚°ì²œë¬¸ëŒ€",
                "title": "ì²œë¬¸ëŒ€ ê²¬í•™ ë° ê´€ì¸¡ì²´í—˜",
                "date": "ë§¤ì£¼ í† ìš”ì¼ (ì˜ˆì•½ í•„ìˆ˜)",
                "location": "ê²½ë¶ ì˜ì²œì‹œ",
                "content": "êµ­ë‚´ ìµœëŒ€ ê·œëª¨ì˜ 1.8m ë§ì›ê²½ ê²¬í•™ê³¼ í•¨ê»˜ ì‹¤ì œ ì²œì²´ ê´€ì¸¡ì„ ì²´í—˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¤‘í•™ìƒ ì´ìƒ ê¶Œì¥í•©ë‹ˆë‹¤.",
                "url": "https://boao.kasi.re.kr"
            }
        ]
        
        all_events = gwacheon_events + regional_events
        
        for event in all_events:
            formatted_content = f"""ğŸ›ï¸ **{event['name']} íŠ¹ë³„ í”„ë¡œê·¸ë¨**

ğŸ¯ **í”„ë¡œê·¸ë¨**: {event['title']}
ğŸ“… **ì¼ì •**: {event['date']}
ğŸ“ **ìœ„ì¹˜**: {event['location']}

ğŸ“ **í”„ë¡œê·¸ë¨ ì†Œê°œ**
{event['content']}

---
ğŸ”— **ìì„¸í•œ ì •ë³´**: {event['url']}
ğŸ“ **ì˜ˆì•½ ë¬¸ì˜**: í•´ë‹¹ ê¸°ê´€ í™ˆí˜ì´ì§€ ì°¸ì¡°
ğŸ·ï¸ **ì¹´í…Œê³ ë¦¬**: ì²œë¬¸ëŒ€/ì²´í—˜í”„ë¡œê·¸ë¨"""

            events.append({
                "title": f"[{event['name']}] {event['title']}",
                "content": formatted_content,
                "source": "Observatory"
            })
        
        logger.info(f"ê³¼í•™ê´€/ì²œë¬¸ëŒ€ ì´ë²¤íŠ¸ {len(events)}ê°œ ì²˜ë¦¬ ì™„ë£Œ")
        return events
        
    except Exception as e:
        logger.error(f"ê³¼í•™ê´€/ì²œë¬¸ëŒ€ ì´ë²¤íŠ¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

def send_to_spring_server(title: str, content: str, content_type: str = "NEWS"):
    """ìŠ¤í”„ë§ ì„œë²„ë¡œ ì½˜í…ì¸  ì „ì†¡"""
    from config import SPRING_SERVER_URL, REQUEST_TIMEOUT
    
    # content ê¸¸ì´ ì œí•œ (2000ì)
    if len(content) > 2000:
        content = content[:1997] + "..."
    
    payload = {
        "title": title, 
        "content": content,
        "type": content_type
    }
    headers = {"Content-Type": "application/json"}
    
    try:
        # Public ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš© (JWT ì¸ì¦ ë¶ˆí•„ìš”)
        response = requests.post(
            f"{SPRING_SERVER_URL}/api/public/ai/news",
            json=payload,
            headers=headers,
            timeout=REQUEST_TIMEOUT
        )
        
        if response.status_code == 200:
            logger.info(f"ì „ì†¡ ì„±ê³µ: {title[:50]}...")
            return True
        else:
            logger.error(f"ì „ì†¡ ì‹¤íŒ¨: {title[:30]}..., ì‘ë‹µ ì½”ë“œ: {response.status_code}, ì‘ë‹µ: {response.text}")
            return False
    except requests.exceptions.Timeout:
        logger.error(f"ì „ì†¡ íƒ€ì„ì•„ì›ƒ: {title[:30]}...")
        return False
    except requests.exceptions.ConnectionError:
        logger.error(f"ì—°ê²° ì˜¤ë¥˜: {title[:30]}... - ìŠ¤í”„ë§ ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”")
        return False
    except Exception as e:
        logger.error(f"ì „ì†¡ ì˜ˆì™¸: {title[:30]}...: {e}")
        return False

async def crawl_all_korean_content():
    """ëª¨ë“  í•œêµ­ ë‰´ìŠ¤ ì†ŒìŠ¤ì™€ ì „êµ­ ì²œë¬¸ëŒ€ì—ì„œ í¬ë¡¤ë§í•˜ì—¬ ìŠ¤í”„ë§ ì„œë²„ë¡œ ì „ì†¡"""
    logger.info(f"í•œêµ­ ì½˜í…ì¸  í¬ë¡¤ë§ ì‹œì‘: {datetime.now()}")
    
    # 1. ë‰´ìŠ¤ í¬ë¡¤ë§
    all_articles = []
    all_articles.extend(crawl_sciencetimes())
    all_articles.extend(crawl_kasi())
    all_articles.extend(crawl_dongascience())
    
    # 2. ê³¼í•™ê´€ ì´ë²¤íŠ¸ í¬ë¡¤ë§
    observatory_events = crawl_science_museums()
    
    # 3. ì „êµ­ ì²œë¬¸ëŒ€ í¬ë¡¤ë§ (ìƒˆë¡œìš´ ì´ë²¤íŠ¸ API ì‚¬ìš©)
    korea_observatories_result = await crawl_all_korea_observatories()
    
    # ë‰´ìŠ¤ ì „ì†¡
    news_success = 0
    for article in all_articles:
        if send_to_spring_server(article["title"], article["content"], "NEWS"):
            news_success += 1
    
    # ê³¼í•™ê´€ ì´ë²¤íŠ¸ ì „ì†¡ (ê¸°ì¡´ ë°©ì‹)
    observatory_success = 0
    for event in observatory_events:
        if send_to_spring_server(event["title"], event["content"], "OBSERVATORY"):
            observatory_success += 1
    
    total_items = len(all_articles) + len(observatory_events) + korea_observatories_result["total_events"]
    total_success = news_success + observatory_success + korea_observatories_result["success"]
    
    logger.info(f"í•œêµ­ ì½˜í…ì¸  í¬ë¡¤ë§ ì™„ë£Œ: ì´ {total_items}ê°œ ì¤‘ {total_success}ê°œ ì „ì†¡ ì„±ê³µ")
    logger.info(f"  - ë‰´ìŠ¤: {len(all_articles)}ê°œ ì¤‘ {news_success}ê°œ ì„±ê³µ")
    logger.info(f"  - ê³¼í•™ê´€ ì´ë²¤íŠ¸: {len(observatory_events)}ê°œ ì¤‘ {observatory_success}ê°œ ì„±ê³µ")
    logger.info(f"  - ì „êµ­ ì²œë¬¸ëŒ€: {korea_observatories_result['total_events']}ê°œ ì¤‘ {korea_observatories_result['success']}ê°œ ì„±ê³µ")
    
    return {
        "total": total_items, 
        "success": total_success,
        "news": {"total": len(all_articles), "success": news_success},
        "science_museums": {"total": len(observatory_events), "success": observatory_success},
        "korea_observatories": korea_observatories_result
    }