import requests
from bs4 import BeautifulSoup
import logging
from datetime import datetime
import urllib3
from crawler.content_extractor import extract_article_content, extract_kasi_article, extract_sciencetimes_article
from crawler.content_formatter import format_news_article
from translator.free_translator import smart_free_translate

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
logger = logging.getLogger(__name__)

def crawl_sciencetimes():
    """ì‚¬ì´ì–¸ìŠ¤íƒ€ì„ì¦ˆ - ëŒ€ì¤‘ ì¹œí™”ì  ìš°ì£¼ê³¼í•™ ë‰´ìŠ¤"""
    try:
        # ìš°ì£¼/ë³‘ê³„ ì¹´í…Œê³ ë¦¬ URL ìˆ˜ì •
        url = "https://www.sciencetimes.co.kr/news/articleList.html?sc_section_code=S1N8&view_type=sm"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        resp = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(resp.content, "html.parser")

        articles = []
        # ì‚¬ì´ì–¸ìŠ¤íƒ€ì„ì¦ˆ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ì…€ë ‰í„° ìˆ˜ì •
        for item in soup.select(".article-list-content")[:3]:
            title_tag = item.select_one(".titles a") or item.select_one("h4 a") or item.select_one(".article-list-titles a")
            if title_tag:
                title = title_tag.get_text(strip=True)
                article_url = title_tag.get('href')
                
                if not article_url.startswith('http'):
                    article_url = "https://www.sciencetimes.co.kr" + article_url
                
                # ê°„ë‹¨í•œ ë‚´ìš© ì¶”ì¶œ
                content = f"ì‚¬ì´ì–¸ìŠ¤íƒ€ì„ì¦ˆì—ì„œ ë³´ë„í•œ ìš°ì£¼ê³¼í•™ ë‰´ìŠ¤ì…ë‹ˆë‹¤. {title}"
                
                articles.append({
                    "title": f"[ì‚¬ì´ì–¸ìŠ¤íƒ€ì„ì¦ˆ] {title}",
                    "content": content + f"\n\nğŸ”— ì›ë¬¸ ë³´ê¸°: {article_url}",
                    "source": "ScienceTimes"
                })
                logger.info(f"ì‚¬ì´ì–¸ìŠ¤íƒ€ì„ì¦ˆ ê¸°ì‚¬ ì²˜ë¦¬ ì™„ë£Œ: {title[:50]}...")
        
        return articles
    except Exception as e:
        logger.error(f"ì‚¬ì´ì–¸ìŠ¤íƒ€ì„ì¦ˆ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []  # ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

def crawl_kasi():
    """í•œêµ­ì²œë¬¸ì—°êµ¬ì› - ê³µì‹ ë°œí‘œ ë° ê¸°ìˆ  ìë£Œ"""
    try:
        url = "https://www.kasi.re.kr/kor/publication/pressRelease"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        resp = requests.get(url, headers=headers, verify=False, timeout=15)
        soup = BeautifulSoup(resp.content, "html.parser")
        
        articles = []
        # KASI ë³´ë„ìë£Œ ì…€ë ‰í„° ìˆ˜ì •
        for item in soup.select(".board_list tbody tr")[:3]:
            title_tag = item.select_one("td.subject a") or item.select_one(".subject a")
            if title_tag:
                title = title_tag.get_text(strip=True)
                article_url = title_tag.get('href')
                
                if not article_url.startswith('http'):
                    full_url = f"https://www.kasi.re.kr{article_url}"
                else:
                    full_url = article_url
                
                # ê°„ë‹¨í•œ ë‚´ìš© ìƒì„±
                content = f"í•œêµ­ì²œë¬¸ì—°êµ¬ì›ì—ì„œ ë°œí‘œí•œ ê³µì‹ ë³´ë„ìë£Œì…ë‹ˆë‹¤. {title}"
                
                articles.append({
                    "title": f"[í•œêµ­ì²œë¬¸ì—°êµ¬ì›] {title}",
                    "content": content + f"\n\nğŸ”— ì›ë¬¸ ë³´ê¸°: {full_url}",
                    "source": "KASI"
                })
                logger.info(f"KASI ê¸°ì‚¬ ì²˜ë¦¬ ì™„ë£Œ: {title[:50]}...")
        
        return articles
    except Exception as e:
        logger.error(f"KASI í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []  # ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

def crawl_dongascience():
    """ë™ì•„ì‚¬ì´ì–¸ìŠ¤ - ìš°ì£¼/ì²œë¬¸ ì„¹ì…˜"""
    try:
        url = "https://www.dongascience.com/news.php?idx=45"
        resp = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
        soup = BeautifulSoup(resp.content, "html.parser")
        
        articles = []
        for item in soup.select(".article_list .list_item")[:3]:
            title_tag = item.select_one(".tit a")
            if title_tag:
                title = title_tag.get_text(strip=True)
                article_url = "https://www.dongascience.com" + title_tag.get('href')
                
                full_content, image_url = extract_article_content(article_url)
                formatted_title, formatted_content = format_news_article(
                    title, full_content, "ë™ì•„ì‚¬ì´ì–¸ìŠ¤", article_url, image_url
                )
                
                articles.append({
                    "title": formatted_title,
                    "content": formatted_content,
                    "source": "DongaScience"
                })
                logger.info(f"ë™ì•„ì‚¬ì´ì–¸ìŠ¤ ê¸°ì‚¬ ì²˜ë¦¬ ì™„ë£Œ: {title[:50]}...")
        return articles
    except Exception as e:
        logger.error(f"ë™ì•„ì‚¬ì´ì–¸ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

def crawl_gwacheon_news():
    """êµ­ë¦½ê³¼ì²œê³¼í•™ê´€ ë‰´ìŠ¤ (ì‹¤ì œ í¬ë¡¤ë§ êµ¬í˜„ í•„ìš”)"""
    try:
        # TODO: ì‹¤ì œ ê³¼ì²œê³¼í•™ê´€ ë‰´ìŠ¤ í˜ì´ì§€ í¬ë¡¤ë§ êµ¬í˜„
        # í˜„ì¬ëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        logger.info("êµ­ë¦½ê³¼ì²œê³¼í•™ê´€ ë‰´ìŠ¤ í¬ë¡¤ë§ ë¯¸êµ¬í˜„")
        return []
    except Exception as e:
        logger.error(f"êµ­ë¦½ê³¼ì²œê³¼í•™ê´€ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

def send_news_to_spring(title: str, content: str, source: str) -> bool:
    """ë‰´ìŠ¤ë¥¼ ìŠ¤í”„ë§ ì„œë²„ë¡œ ì „ì†¡ (NEWS ì¹´í…Œê³ ë¦¬)"""
    from utils.simple_sender import send_to_spring
    
    if len(content) > 2000:
        content = content[:1997] + "..."
    
    payload = {
        "title": title, 
        "content": content,
        "type": "NEWS",
        "authorId": "newsbot",
        "category": "NEWS",
        "source": source
    }
    
    return send_to_spring(payload, "/api/public/ai/news", source)

async def crawl_news_only():
    """ìš°ì£¼ ë‰´ìŠ¤ë§Œ í¬ë¡¤ë§ (í•˜ë£¨ 2íšŒ: ì˜¤ì „ 6ì‹œ, ì˜¤í›„ 12ì‹œ)"""
    logger.info(f"ìš°ì£¼ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘: {datetime.now()}")
    
    all_articles = []
    
    # í•œêµ­ì¸ ë§ì¶¤ ìš°ì£¼ ë‰´ìŠ¤
    try:
        from crawler.korean_space_news import get_korean_space_news
        korean_articles = get_korean_space_news()
        all_articles.extend(korean_articles)
        logger.info(f"í•œêµ­ ìš°ì£¼ ë‰´ìŠ¤ {len(korean_articles)}ê°œ ìˆ˜ì§‘")
    except Exception as e:
        logger.error(f"í•œêµ­ ìš°ì£¼ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    
    # ê¸°ì¡´ ì†ŒìŠ¤ë„ ìœ ì§€
    all_articles.extend(crawl_sciencetimes())
    all_articles.extend(crawl_kasi())
    
    # ë°±ì—… ë‰´ìŠ¤ (í¬ë¡¤ë§ì´ ì•ˆë  ë•Œ)
    if len(all_articles) == 0:
        try:
            from crawler.simple_news import get_simple_space_news
            backup_articles = get_simple_space_news()
            all_articles.extend(backup_articles)
            logger.info(f"ë°±ì—… ë‰´ìŠ¤ {len(backup_articles)}ê°œ ì‚¬ìš©")
        except Exception as e:
            logger.error(f"ë°±ì—… ë‰´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    success_count = 0
    for article in all_articles:
        if send_news_to_spring(article["title"], article["content"], article["source"]):
            success_count += 1
    
    logger.info(f"ìš°ì£¼ ë‰´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_articles)}ê°œ ì¤‘ {success_count}ê°œ ì „ì†¡ ì„±ê³µ")
    
    return {
        "total": len(all_articles), 
        "success": success_count,
        "sources": ["YTNì‚¬ì´ì–¸ìŠ¤", "ì¡°ì„ ì¼ë³´", "ì‚¬ì´ì–¸ìŠ¤ì˜¨", "ì‚¬ì´ì–¸ìŠ¤íƒ€ì„ì¦ˆ", "KASI"]
    }